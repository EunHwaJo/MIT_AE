lm(gpm~factor(am)*wt,data=mtcars)
summary(lm(gpm~factor(am)*wt,data=mtcars))
summary(lm(gpm~factor(am)*wt-1,data=mtcars))
summary(lm(gpm~factor(am)+wt-1,data=mtcars))
summary(lm(gpm~factor(am)+wt,data=mtcars))
g2<-ggplot(data=mtcars,aes(x=wt,y=gpm))+
geom_point()+
geom_smooth(method=lm)+
#facet_wrap(~amf,shrink=FALSE)+
labs(x = "Weight / 1000 lb",y = "Gallons per 100 miles")
g2
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2))
logit<-ln(P/(1-P))
logit
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2))
logit<-log(P/(1-P))
logit
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2))
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2)) # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-(beta0+beta1*x1+beta2*x2))_ # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-(beta0+beta1*x1+beta2*x2))) # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
tb<-data.frame("Allocation"=c("Pay costs","Devolved to depts. for CPD","Central Funding"),c(27,33,39))
library(ggplot2)
tb<-data.frame("Category"=c("Pay costs","Devolved to depts. for CPD","Central Funding"),"Allocation"=c(27,33,39))
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,colour=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
# theme(legend.text=element_text(size=12),
legend.title = element_blank())+
#theme(legend.position=c(.8, .2)
)
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2)
)
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=18))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=18))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())+
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
theme(legend.position="none")
d
source('C:/Users/Mike/Rspace/RECM/thermalMass.r', echo=TRUE)
library(swirl)
swirl()
var(rpois(1000,50))
nxt()
head(hits)
class(hits[,"date'])
)
class(hits[,"date"])
as.integer(head(hits[,"data"]))
as.integer(head(hits[,"date"]))
mdl<-glm(visits~date,family=poisson,data=hits)
summary(mdl)
exp(confint(mdl,"date"))
which.max(hits[,"visits"])
hits[704,]
lambda<-mdl$fitted.values[704]
qpois(0.95,lambda)
mdl2<-offset=log(visits+1)
mdl2<-log(visits+1)
mdl2<-glm$offset
?
setwd("C:/Users/Mike/Rspace/MIT_AE/Unit7")
tweets=read.csv("./data/tweets.csv",stringsAsFactors=FALSE)
tweets=read.csv("./data/tweets.csv",stringsAsFactors=FALSE)
# pre-processing of the data
library(tm)
corpus = Corpus(VectorSource(tweets$tweet))
# Pre-process data
corpus = tm_map(corpus, tolower)
# IMPORTANT NOTE: If you are using the latest version of the tm package, you will need to run the following line before continuing (it converts corpus to a Plain Text Document). This is a recent change having to do with the tolower function that occurred after this video was recorded.
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
length(stopwords("english"))
#create a document term matrix
dtm = DocumentTermMatrix(corpus)
# 2.3 Create data frame
allTweets = as.data.frame(as.matrix(dtm))
names(allTweets)<-make.names(names(allTweets))
# which word occurs most frequently
library(dplyr)
sums<-as.numeric(colSums(allTweets))
names<-names(allTweets)
Absums<-data.frame(names,sums)
arrange(Absums,-sums)[1,]
# pre-processing of the data
str(tweets)
rm(list=ls())
## THE ANALYTICS EDGE 15.071x
## UNIT 7 A3
## Michael Hunt
## July 2015
## VISUALISING TEXT DATA USING WORD CLOUDS
# download the data
tweets=read.csv("./data/tweets.csv",stringsAsFactors=FALSE)
# pre-processing of the data
library(tm)
corpus = Corpus(VectorSource(tweets$Tweet))
# Pre-process data
corpus = tm_map(corpus, tolower)
# IMPORTANT NOTE: If you are using the latest version of the tm package, you will need to run the following line before continuing (it converts corpus to a Plain Text Document). This is a recent change having to do with the tolower function that occurred after this video was recorded.
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
length(stopwords("english"))
#create a document term matrix
dtm = DocumentTermMatrix(corpus)
# 2.3 Create data frame
allTweets = as.data.frame(as.matrix(dtm))
names(allTweets)<-make.names(names(allTweets))
# which word occurs most frequently
library(dplyr)
sums<-as.numeric(colSums(allTweets))
names<-names(allTweets)
Absums<-data.frame(names,sums)
arrange(Absums,-sums)[1,]
arrange(Absums,-sums)[1:10,]
nrow(Absums)
tail(Absums)
str(allTweets)
str(tweets)
str(allTweets)
install.packages("wordcloud")
library(wordcloud)
head(rownames(allTweets))
head(colnames(allTweets))
head(names(allTweets))
head(names)
library(dplyr)
sums<-as.numeric(colSums(allTweets))
names<-names(allTweets)
Absums<-data.frame(names,sums)
arrange(Absums,-sums)[1:10,]
head(Absums)
tail(Absums)
words<-colnames(allTweets)
words[100:120]
?wordcloud
wordcloud(names,sums)
arrange(Absums,-sums)[1:10,]
wordcloud(names,sums,scale=c(2, 0.25)
wordcloud(names,sums,scale=c(2, 0.25))
wordcloud(names,sums,scale=c(3, 0.375))
length(stopwords("english"))
head(stopwords("english"))
?tm_map
?removeWords
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english"))
#create a document term matrix
dtm = DocumentTermMatrix(corpus)
#  Create data frame
allTweets = as.data.frame(as.matrix(dtm))
names(allTweets)<-make.names(names(allTweets))
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english")))
#create a document term matrix
dtm = DocumentTermMatrix(corpus)
#  Create data frame
allTweets = as.data.frame(as.matrix(dtm))
names(allTweets)<-make.names(names(allTweets))
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english")))
#create a document term matrix
dtm = DocumentTermMatrix(corpus)
#  Create data frame
allTweets = as.data.frame(as.matrix(dtm))
names(allTweets)<-make.names(names(allTweets))
library(dplyr)
sums<-as.numeric(colSums(allTweets))
names<-names(allTweets)
Absums<-data.frame(names,sums)
arrange(Absums,-sums)[1:10,]
# load wordcloud package
library(wordcloud)
#build the new wordcloud, now excluding "apple"
wordcloud(names,sums,scale=c(3, 0.375))
# compare with
arrange(Absums,-sums)[1:10,]
wordcloud(names,sums,scale=c(2, 0.25))
?wordcloud
head(allTweets)
str(allTweets)
str(tweets)
negtweets=subset(tweets,Avg<=-1)
negtweets=subset(tweets,Avg<=-1)
ncorpus = Corpus(VectorSource(negtweets$Tweet))
ncorpus = tm_map(ncorpus, tolower)
ncorpus = tm_map(ncorpus, PlainTextDocument)
ncorpus = tm_map(ncorpus, removePunctuation)
ncorpus = tm_map(ncorpus, removeWords, c("apple",stopwords("english")))
#create a document term matrix
negdtm = DocumentTermMatrix(ncorpus)
#  Create data frame
negTweets = as.data.frame(as.matrix(negdtm))
names(negTweets)<-make.names(names(negTweets))
library(dplyr)
negsums<-as.numeric(colSums(negTweets))
negnames<-names(negTweets)
NegAbsums<-data.frame(negnames,negsums)
arrange(NegAbsums,-negsums)[1:10,]
# load wordcloud package
library(wordcloud)
#build the new wordcloud, now excluding "apple"
wordcloud(negnames,negsums,scale=c(2, 0.25))
# compare with
arrange(NegAbsums,-negsums)[1:10,]
negtweets=subset(tweets,Avg<=-1)
ncorpus = Corpus(VectorSource(negtweets$Tweet))
ncorpus = tm_map(ncorpus, tolower)
ncorpus = tm_map(ncorpus, PlainTextDocument)
ncorpus = tm_map(ncorpus, removePunctuation)
ncorpus = tm_map(ncorpus, removeWords, c("apple",stopwords("english")))
#create a document term matrix
negdtm = DocumentTermMatrix(ncorpus)
#  Create data frame
negTweets = as.data.frame(as.matrix(negdtm))
names(negTweets)<-make.names(names(negTweets))
library(dplyr)
negsums<-as.numeric(colSums(negTweets))
negnames<-names(negTweets)
NegAbsums<-data.frame(negnames,negsums)
arrange(NegAbsums,-negsums)[1:10,]
# load wordcloud package
library(wordcloud)
#build the new wordcloud, now excluding "apple"
wordcloud(negnames,negsums,scale=c(2, 0.25))
# compare with
arrange(NegAbsums,-negsums)[1:10,]
negativeTweets = subset(allTweets, tweets$Avg <= -1)
wordcloud(colnames(negativeTweets), colSums(negativeTweets))
?wordcloud
tail(Absums(1:50))
tail(Absums[1:50])
tail(Absums[1:50,])
?head
tail(Absums)
wordcloud(names,sums,scale=c(2, 0.25))
wordcloud(names,sums,scale=c(2, 0.25),min.freq=5)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,maxwords=100)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,maxwords=50)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,max.words=50)
warnings()
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,max.words=50)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,max.words=100)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,max.words=1000)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=2,max.words=200)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=5,max.words=1000)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=10,max.words=1000)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=10,max.words=1000,random.order=FALSE)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=10,max.words=1000,random.order=TRUE)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=10,max.words=1000,random.order=TRUE)
wordcloud(names,sums,scale=c(2, 0.25),min.freq=10,max.words=1000,random.order=FALSE)
wordcloud(names,sums,scale=c(2, 0.25))
wordcloud(names,sums,scale=c(2, 0.5))
install.package("RColorBrewer")
install.packages("RColorBrewer")
load(RColorBrewer)
library(RColorBrewer)
?brewer.pal()
display.brewer.all()
brewer.pal(9, "Blues")[c(5, 6, 7, 8, 9)]
brewer.pal(9, "Blues")[c(-1, -2, -3, -4)]
brewer.pal(9, "Blues")
wordcloud(names,sums,scale=c(2, 0.25),colors=brewer.pal(9, "Blues"))
wordcloud(names,sums,scale=c(2, 0.25),colors=brewer.pal(9, "Blues")[c(5:9)])
wordcloud(names,sums,scale=c(2, 0.25),max.words=100,colors=brewer.pal(9, "Blues")[c(5:9)])
wordcloud(names,sums,scale=c(2, 0.25),min.freq=4,colors=brewer.pal(9, "Blues")[c(5:9)])
wordcloud(names,sums,scale=c(2, 0.25),min.freq=4,colors=brewer.pal(9, "YlorRd")[c(1:9)])
wordcloud(names,sums,scale=c(2, 0.25),min.freq=4,colors=brewer.pal(9, "YlOrRd")[c(1:9)])
wordcloud(names,sums,scale=c(2, 0.25),min.freq=4,colors=brewer.pal(9, "YlOrRd")[c(5:9)])
setwd("C:/Users/Mike/Rspace/MIT_AE/Kaggle")
sample<-read.csv("./data/SampleSubmission.csv")
sample<-read.csv("./data/SampleSubmission.csv")
rm(list=ls())
sample<-read.csv("./data/SampleSubmission.csv")
head(sample)
sample<-read.csv("./data/SampleSubmission.csv")
head(sample)
eBayTrain = read.csv("./data/eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("./data/eBayiPadTest.csv", stringsAsFactors=FALSE)
SimpleMod = glm(sold ~ startprice, data=eBayTrain, family=binomial)
PredTest = predict(SimpleMod, newdata=eBayTest, type="response")
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
