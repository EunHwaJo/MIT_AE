x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2))
logit<-log(P/(1-P))
logit
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2))
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-beta0+beta1*x1+beta2*x2)) # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-(beta0+beta1*x1+beta2*x2))_ # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
beta0=-1.5
beta1=3
beta2=-0.5
x1=1
x2=5
P<-1/(1+exp(-(beta0+beta1*x1+beta2*x2))) # =P(y=1)
logit<-log(P/(1-P))
logit
odds<-exp(logit)
odds
P # =P(y=1)
tb<-data.frame("Allocation"=c("Pay costs","Devolved to depts. for CPD","Central Funding"),c(27,33,39))
library(ggplot2)
tb<-data.frame("Category"=c("Pay costs","Devolved to depts. for CPD","Central Funding"),"Allocation"=c(27,33,39))
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,colour=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
theme(legend.text=element_text(size=12),
legend.title = element_blank())+
theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())+
# theme(legend.text=element_text(size=12),
legend.title = element_blank())+
#theme(legend.position=c(.8, .2)
)
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2)
)
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=14),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=18))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=14,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=18))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
#theme(legend.position=c(.8, .2))
d
d<-ggplot(data=tb,
aes(x=Category, y=Allocation,fill=Allocation, order=-Allocation))+
geom_bar(stat="identity")+
#facet_wrap(~variable)+
coord_flip()+
scale_y_continuous(breaks = seq(0, 50, 10))+
theme(axis.text.x = element_text(size=18),
axis.text.y=element_text(size=14))+
labs(y = "% Allocation of total training budget")+
theme(axis.title.x = element_text(size=18,vjust=-.5),
axis.title.y=element_blank())+
# theme(legend.text=element_text(size=12),
# legend.title = element_blank())+
theme(legend.position="none")
d
# Unit 5 - Recitation
# Video 2
# Load the dataset
emails = read.csv("./data/energy_bids.csv", stringsAsFactors=FALSE)
str(emails)
# Look at emails
emails$email[1]
emails$responsive[1]
emails$email[2]
emails$responsive[2]
# Responsive emails
table(emails$responsive)
# Video 3
# Load tm package
library(tm)
# Create corpus
corpus = Corpus(VectorSource(emails$email))
corpus[[1]]
# Pre-process data
corpus = tm_map(corpus, tolower)
# IMPORTANT NOTE: If you are using the latest version of the tm package, you will need to run the following line before continuing (it converts corpus to a Plain Text Document). This is a recent change having to do with the tolower function that occurred after this video was recorded.
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
# Look at first email
corpus[[1]]
length(stopwords("english"))
# Video 4
# Create matrix
dtm = DocumentTermMatrix(corpus)
dtm
# Remove sparse terms
dtm = removeSparseTerms(dtm, 0.97)
dtm
# Create data frame
labeledTerms = as.data.frame(as.matrix(dtm))
# Add in the outcome variable
labeledTerms$responsive = emails$responsive
str(labeledTerms)
# Video 5
# Split the data
library(caTools)
set.seed(144)
spl = sample.split(labeledTerms$responsive, 0.7)
train = subset(labeledTerms, spl == TRUE)
test = subset(labeledTerms, spl == FALSE)
# Build a CART model
library(rpart)
library(rpart.plot)
emailCART = rpart(responsive~., data=train, method="class")
prp(emailCART)
# Video 6
# Make predictions on the test set
pred = predict(emailCART, newdata=test)
pred[1:10,]
pred.prob = pred[,2]
# Compute accuracy
table(test$responsive, pred.prob >= 0.5)
(195+25)/(195+25+17+20)
# Baseline model accuracy
table(test$responsive)
215/(215+42)
# Video 7
# ROC curve
library(ROCR)
predROCR = prediction(pred.prob, test$responsive)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
# Compute AUC
performance(predROCR, "auc")@y.values
setwd("C:/Users/Mike/Rspace/MIT_AE/Unit5")
# Unit 5 - Recitation
# Video 2
# Load the dataset
emails = read.csv("./data/energy_bids.csv", stringsAsFactors=FALSE)
str(emails)
# Look at emails
emails$email[1]
emails$responsive[1]
emails$email[2]
emails$responsive[2]
# Responsive emails
table(emails$responsive)
# Video 3
# Load tm package
library(tm)
# Create corpus
corpus = Corpus(VectorSource(emails$email))
corpus[[1]]
# Pre-process data
corpus = tm_map(corpus, tolower)
# IMPORTANT NOTE: If you are using the latest version of the tm package, you will need to run the following line before continuing (it converts corpus to a Plain Text Document). This is a recent change having to do with the tolower function that occurred after this video was recorded.
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
# Look at first email
corpus[[1]]
length(stopwords("english"))
# Video 4
# Create matrix
dtm = DocumentTermMatrix(corpus)
dtm
# Remove sparse terms
dtm = removeSparseTerms(dtm, 0.97)
dtm
# Create data frame
labeledTerms = as.data.frame(as.matrix(dtm))
# Add in the outcome variable
labeledTerms$responsive = emails$responsive
str(labeledTerms)
# Video 5
# Split the data
library(caTools)
set.seed(144)
spl = sample.split(labeledTerms$responsive, 0.7)
train = subset(labeledTerms, spl == TRUE)
test = subset(labeledTerms, spl == FALSE)
# Build a CART model
library(rpart)
library(rpart.plot)
emailCART = rpart(responsive~., data=train, method="class")
prp(emailCART)
# Video 6
# Make predictions on the test set
pred = predict(emailCART, newdata=test)
pred[1:10,]
pred.prob = pred[,2]
# Compute accuracy
table(test$responsive, pred.prob >= 0.5)
(195+25)/(195+25+17+20)
# Baseline model accuracy
table(test$responsive)
215/(215+42)
# Video 7
# ROC curve
library(ROCR)
predROCR = prediction(pred.prob, test$responsive)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
# Compute AUC
performance(predROCR, "auc")@y.values
stopwords("english")
setwd("C:/Users/Mike/Rspace/MIT_AE/Unit5")
wiki = read.csv("./data/wiki.csv", stringsAsFactors=FALSE)
as.factor(wiki$Vandal)
str(wiki)
sum(wiki$Vandal)
library(tm)
# Create corpus
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded[[1]]
# Pre-process data (laready in lower case and with punc removed)
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
# Create matrix
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
length(stopwords("english"))
sparseAdded = removeSparseTerms(dtm, 0.997)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(dsparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
head(wordsAdded)
library(tm)
# Create corpus
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved[[1]]
# Pre-process data (laready in lower case and with punc removed)
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
# Create matrix
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
# 1.3
# Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions,
# and call the new matrix sparseAdded.
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
# 1.4 convert to data frame
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved)) # prepend all words with "R"
str(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
spl = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(train$Vandal)
# baseline accuracy on test set
table(test$Vandal)
table(test$Vandal)
table(test$Vandal)[1,1]/sum(table(test$Vandal))
table(test$Vandal)
table(test$Vandal)[1]/sum(table(test$Vandal))
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal~., data=train, method="class")
prp(wikiCART)
# Make predictions on the test set
pred = predict(wikiCART, newdata=test,type="class")
#pred[1:10,]
pred.prob = pred[,2]
# Compute accuracy on test set
ct<-table(test$Vandal, pred.prob >= 0.5)
ct
sum(diag(ct))/sum(ct)
pred = predict(wikiCART, newdata=test,type="class")
pred[1:10,]
pred[1:10]
pred = predict(wikiCART, newdata=test,type="class")
#pred[1:10,]
#pred.prob = pred[,2]
# Compute accuracy on test set
ct<-table(test$Vandal, pred >= 0.5)
ct
sum(diag(ct))/sum(ct)
ct<-table(test$Vandal)
ct
pred = predict(wikiCART, newdata=test)
pred[1:10,]
pred.prob = pred[,2]
ct<-table(test$Vandal,pred.prob >= 0.5)
ct
sum(diag(ct))/sum(ct)
prp(wikiCART)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
sum(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART2 = rpart(Vandal~., data=wikiTrain2, method="class")
# Make predictions on the test set
pred2 = predict(wikiCART2, newdata=wikiTest2)
pred2[1:10,]
pred2.prob = pred2[,2]
# Compute accuracy on test set
ct2<-table(wikiTest2$Vandal,pred2.prob >= 0.5)
ct2
sum(diag(ct2))/sum(ct2)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCART2 = rpart(Vandal~., data=wikiTrain2, method="class")
# Make predictions on the test set
pred2 = predict(wikiCART2, newdata=wikiTest2)
pred2[1:10,]
pred2.prob = pred2[,2]
# Compute accuracy on test set
ct2<-table(wikiTest2$Vandal,pred2.prob >= 0.5)
ct2
sum(diag(ct2))/sum(ct2)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain2 = subset(wikiWords3, spl==TRUE)
wikiTest2 = subset(wikiWords3, spl==FALSE)
wikiCART2 = rpart(Vandal~., data=wikiTrain2, method="class")
# Make predictions on the test set
pred2 = predict(wikiCART2, newdata=wikiTest2)
pred2[1:10,]
pred2.prob = pred2[,2]
# Compute accuracy on test set - vastly improved
ct2<-table(wikiTest2$Vandal,pred2.prob >= 0.5)
ct2
sum(diag(ct2))/sum(ct2)
prp(wikiCART2)
